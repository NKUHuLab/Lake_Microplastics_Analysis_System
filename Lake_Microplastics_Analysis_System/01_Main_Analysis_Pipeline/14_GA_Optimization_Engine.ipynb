{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bc31c5-f12c-4a5c-919e-88d28cfa2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- 1. 导入库并加载必要文件 --------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import geatpy as ea\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from tqdm import trange\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# --- 配置区 (请根据您的环境修改路径) ---\n",
    "MODEL_PATH = r'C:\\Users\\DELL\\Desktop\\wrq\\lake_mp_optimization_project\\model\\random_forest_model.pkl'\n",
    "CONSTRAINTS_PATH = r'C:\\Users\\DELL\\Desktop\\wrq\\lake_mp_optimization_project\\data\\yueshu\\final_lake_constraints.csv'\n",
    "OUTPUT_DIR = r'C:\\Users\\DELL\\Desktop\\wrq' # 使用新的最终版输出目录\n",
    "\n",
    "# --- 创建输出目录 (目录保持原名，文件名将包含标记) ---\n",
    "OUTPUT_DIR_Y = os.path.join(OUTPUT_DIR, '最优目标值')\n",
    "OUTPUT_DIR_X = os.path.join(OUTPUT_DIR, '最优解特征')\n",
    "OUTPUT_DIR_ENTROPY = os.path.join(OUTPUT_DIR, '信息熵分析')\n",
    "OUTPUT_DIR_CONTRIBUTION = os.path.join(OUTPUT_DIR, '贡献度分析')\n",
    "os.makedirs(OUTPUT_DIR_Y, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_X, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_ENTROPY, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_CONTRIBUTION, exist_ok=True)\n",
    "\n",
    "# --- 加载模型和数据 (静默模式) ---\n",
    "# print(\"正在加载模型和数据...\")\n",
    "model = joblib.load(MODEL_PATH)\n",
    "data = pd.read_csv(CONSTRAINTS_PATH)\n",
    "# print(\"加载完成！\")\n",
    "\n",
    "# --- 1.1 逐行读取所有60个特征 (优化为列表推导式以减少内存占用) ---\n",
    "# print(\"正在逐一读取所有约束列...\")\n",
    "# 通过列表推导式直接生成列表的列表，避免创建60个中间变量\n",
    "all_cols_list = [data.iloc[:, j].tolist() for j in range(60)]\n",
    "# print(\"列数据读取完成！\")\n",
    "\n",
    "DIMENSION = 22 # 模型需要前22个特征\n",
    "\n",
    "# -------------------- 2. 定义Geatpy优化问题类 --------------------\n",
    "\n",
    "class MyProblemStrict(ea.Problem):\n",
    "    def __init__(self, lb, ub, all_fixed_cols, row_index, model_predictor):\n",
    "        name = 'StrictIndexProblem'; M = 1; maxormins = [1]; varTypes = [0] * DIMENSION\n",
    "        ea.Problem.__init__(self, name, M, maxormins, DIMENSION, varTypes, lb, ub)\n",
    "        self.all_cols = all_fixed_cols; self.row_index = row_index; self.model = model_predictor\n",
    "\n",
    "    def aimFunc(self, pop):\n",
    "        Vars = pop.Phen; i = self.row_index\n",
    "        safe_vars = np.nan_to_num(Vars, nan=0.0)\n",
    "        pop.ObjV = self.model.predict(safe_vars).reshape(-1, 1)\n",
    "        epsilon = 1e-6\n",
    "        ww_init_sum = np.nansum([self.all_cols[6][i], self.all_cols[7][i], self.all_cols[8][i]])\n",
    "        rse_init_sum = np.nansum([self.all_cols[9][i], self.all_cols[10][i], self.all_cols[11][i]])\n",
    "        ww_sec_adv_upper = np.nan_to_num(self.all_cols[50][i], nan=ww_init_sum)\n",
    "        rse_paved_gravel_upper = np.nan_to_num(self.all_cols[53][i], nan=rse_init_sum)\n",
    "        cv1 = np.abs((Vars[:, 6] + Vars[:, 7] + Vars[:, 8]) - ww_init_sum) - epsilon\n",
    "        cv2 = np.abs((Vars[:, 9] + Vars[:, 10] + Vars[:, 11]) - rse_init_sum) - epsilon\n",
    "        cv3 = (Vars[:, 7] + Vars[:, 8]) - ww_sec_adv_upper\n",
    "        cv4 = (Vars[:, 9] + Vars[:, 10]) - rse_paved_gravel_upper\n",
    "        pop.CV = np.hstack([cv1.reshape(-1, 1), cv2.reshape(-1, 1), cv3.reshape(-1, 1), cv4.reshape(-1, 1)])\n",
    "\n",
    "# -------------------- 3. 主循环与优化过程 --------------------\n",
    "results_Y, results_X, results_entropy, results_contribution = [], [], [], []\n",
    "row_ids = data.index.tolist()\n",
    "\n",
    "# --- 修改点：设定处理行数和起始行 ---\n",
    "# Set the starting row index for this run\n",
    "START_ROW = 15000 # Start from the 15,001st row (index 15000)\n",
    "# Set the number of rows to process in this run\n",
    "NUM_ROWS_TO_PROCESS = 15000\n",
    "# Calculate the ending row index, ensuring it doesn't exceed total data length\n",
    "END_ROW = min(START_ROW + NUM_ROWS_TO_PROCESS, len(data))\n",
    "\n",
    "# Loop from START_ROW up to (but not including) END_ROW\n",
    "for i in trange(START_ROW, END_ROW, desc=f\"最终版逐行优化(行 {START_ROW} 到 {END_ROW-1})\"):\n",
    "\n",
    "    # 防御层 1: 检查核心特征，如果含NaN或inf则跳过整行\n",
    "    model_input_features = [all_cols_list[j][i] for j in range(DIMENSION)]\n",
    "    if any(pd.isna(v) or np.isinf(v) for v in model_input_features):\n",
    "        results_Y.append(np.nan); results_X.append([np.nan] * DIMENSION)\n",
    "        results_entropy.append([np.nan] * DIMENSION); results_contribution.append([np.nan] * DIMENSION)\n",
    "        continue\n",
    "\n",
    "    lb = np.zeros(DIMENSION); ub = np.zeros(DIMENSION)\n",
    "\n",
    "    # 防御层 2: 逐个变量设置边界，如果约束依赖含NaN或inf则固定变量\n",
    "    # 1. 固定变量\n",
    "    for col_idx in [0, 1, 2, 3, 4, 5, 12, 13, 14, 15, 16, 18]:\n",
    "        val = all_cols_list[col_idx][i]\n",
    "        lb[col_idx] = val\n",
    "        ub[col_idx] = val\n",
    "\n",
    "    # 2. 可优化变量\n",
    "    # 定义一个辅助函数来检查依赖项\n",
    "    def check_deps(dep_list):\n",
    "        return any(pd.isna(v) or np.isinf(v) for v in dep_list)\n",
    "\n",
    "    # Primary_Waste_Discharge (idx 6)\n",
    "    deps_6 = [all_cols_list[6][i], all_cols_list[7][i], all_cols_list[8][i], all_cols_list[51][i]]\n",
    "    if check_deps(deps_6):\n",
    "        lb[6] = ub[6] = all_cols_list[6][i]\n",
    "    else:\n",
    "        lb[6] = max(0, all_cols_list[51][i]); ub[6] = sum(deps_6[0:3])\n",
    "\n",
    "    # Secondary_Waste_Discharge (idx 7)\n",
    "    deps_7 = [all_cols_list[6][i], all_cols_list[7][i], all_cols_list[8][i]]\n",
    "    if check_deps(deps_7):\n",
    "        lb[7] = ub[7] = all_cols_list[7][i]\n",
    "    else:\n",
    "        lb[7] = max(0, all_cols_list[7][i]); ub[7] = sum(deps_7)\n",
    "\n",
    "    # Advanced_Waste_Discharge (idx 8)\n",
    "    deps_8 = [all_cols_list[8][i], all_cols_list[49][i]]\n",
    "    if check_deps(deps_8):\n",
    "        lb[8] = ub[8] = all_cols_list[8][i]\n",
    "    else:\n",
    "        lb[8] = max(0, all_cols_list[8][i]); ub[8] = all_cols_list[49][i]\n",
    "\n",
    "    # RSE_paved (idx 9)\n",
    "    deps_9 = [all_cols_list[9][i], all_cols_list[52][i]]\n",
    "    if check_deps(deps_9):\n",
    "        lb[9] = ub[9] = all_cols_list[9][i]\n",
    "    else:\n",
    "        lb[9] = max(0, all_cols_list[9][i]); ub[9] = all_cols_list[52][i]\n",
    "\n",
    "    # RSE_gravel (idx 10)\n",
    "    deps_10 = [all_cols_list[9][i], all_cols_list[10][i], all_cols_list[11][i]]\n",
    "    if check_deps(deps_10):\n",
    "        lb[10] = ub[10] = all_cols_list[10][i]\n",
    "    else:\n",
    "        lb[10] = max(0, all_cols_list[10][i]); ub[10] = sum(deps_10)\n",
    "\n",
    "    # RSE_other (idx 11)\n",
    "    deps_11 = [all_cols_list[9][i], all_cols_list[10][i], all_cols_list[11][i], all_cols_list[54][i]]\n",
    "    if check_deps(deps_11):\n",
    "        lb[11] = ub[11] = all_cols_list[11][i]\n",
    "    else:\n",
    "        lb[11] = max(0, all_cols_list[54][i]); ub[11] = sum(deps_11[0:3])\n",
    "\n",
    "    # Mismanaged (idx 17)\n",
    "    deps_17 = [all_cols_list[17][i], all_cols_list[55][i]]\n",
    "    if check_deps(deps_17) or all_cols_list[17][i] == 0:\n",
    "        lb[17] = ub[17] = all_cols_list[17][i]\n",
    "    else:\n",
    "        lb[17] = max(0, all_cols_list[55][i]); ub[17] = all_cols_list[17][i]\n",
    "\n",
    "    # fish_gdp_sqkm (idx 19)\n",
    "    deps_19 = [all_cols_list[19][i], all_cols_list[56][i], all_cols_list[57][i]]\n",
    "    if check_deps(deps_19):\n",
    "        lb[19] = ub[19] = all_cols_list[19][i]\n",
    "    else:\n",
    "        lb[19] = max(0, all_cols_list[19][i] * all_cols_list[56][i])\n",
    "        ub[19] = all_cols_list[19][i] * all_cols_list[57][i]\n",
    "\n",
    "    # Cultivated_land (idx 20)\n",
    "    deps_20 = [all_cols_list[20][i], all_cols_list[31][i], all_cols_list[32][i]]\n",
    "    if check_deps(deps_20):\n",
    "        lb[20] = ub[20] = all_cols_list[20][i]\n",
    "    else:\n",
    "        lb[20] = max(0, all_cols_list[31][i]); ub[20] = all_cols_list[32][i]\n",
    "\n",
    "    # Artificial_surface (idx 21)\n",
    "    deps_21 = [all_cols_list[21][i], all_cols_list[33][i], all_cols_list[34][i]]\n",
    "    if check_deps(deps_21):\n",
    "        lb[21] = ub[21] = all_cols_list[21][i]\n",
    "    else:\n",
    "        lb[21] = max(0, all_cols_list[33][i]); ub[21] = all_cols_list[34][i]\n",
    "\n",
    "    # 最终强制校正，以防浮点数精度导致 lb > ub\n",
    "    fix_indices = np.where(lb > ub)[0]\n",
    "    if len(fix_indices) > 0:\n",
    "        lb[fix_indices] = ub[fix_indices]\n",
    "\n",
    "    if np.any(lb > ub) or np.any(np.isnan(lb)) or np.any(np.isnan(ub)) or np.any(np.isinf(lb)) or np.any(np.isinf(ub)):\n",
    "        results_Y.append(np.nan); results_X.append([np.nan] * DIMENSION)\n",
    "        results_entropy.append([np.nan] * DIMENSION); results_contribution.append([np.nan] * DIMENSION)\n",
    "        continue\n",
    "\n",
    "    problem = MyProblemStrict(lb=lb, ub=ub, all_fixed_cols=all_cols_list, row_index=i, model_predictor=model)\n",
    "    algorithm = ea.soea_EGA_templet(problem, ea.Population(Encoding='RI', NIND=50), MAXGEN=100, logTras=0)\n",
    "    res = ea.optimize(algorithm, seed=1, verbose=False, drawing=0, outputMsg=False)\n",
    "\n",
    "    if res['success']:\n",
    "        best_y = res['optPop'].ObjV[0, 0]; best_x = res['optPop'].Phen[0, :]\n",
    "        final_pop_vars = res['optPop'].Phen\n",
    "        results_Y.append(best_y); results_X.append(best_x)\n",
    "        entropies = [entropy(np.histogram(final_pop_vars[:, j], bins=10)[0] / final_pop_vars.shape[0]) for j in range(DIMENSION)]\n",
    "        results_entropy.append(entropies)\n",
    "        results_contribution.append(model.feature_importances_)\n",
    "    else:\n",
    "        results_Y.append(np.nan); results_X.append([np.nan] * DIMENSION)\n",
    "        results_entropy.append([np.nan] * DIMENSION); results_contribution.append([np.nan] * DIMENSION)\n",
    "\n",
    "\n",
    "# -------------------- 4. 保存结果到文件 --------------------\n",
    "# print(\"\\n优化完成，正在保存结果...\")\n",
    "\n",
    "# 定义特征名称以便于输出文件有表头\n",
    "feature_names = [f'feature_{j}' for j in range(DIMENSION)]\n",
    "# Modified file suffix to reflect the processed range\n",
    "file_suffix = f\"_{START_ROW}_to_{END_ROW-1}\"\n",
    "\n",
    "# 保存最优目标值Y\n",
    "df_Y = pd.DataFrame(results_Y, columns=['optimal_target_value'])\n",
    "df_Y.to_csv(os.path.join(OUTPUT_DIR_Y, f'optimal_Y{file_suffix}.csv'), index=False)\n",
    "\n",
    "# 保存最优解X\n",
    "df_X = pd.DataFrame(results_X, columns=feature_names)\n",
    "df_X.to_csv(os.path.join(OUTPUT_DIR_X, f'optimal_X{file_suffix}.csv'), index=False)\n",
    "\n",
    "# 保存信息熵分析\n",
    "df_entropy = pd.DataFrame(results_entropy, columns=[f'entropy_{j}' for j in range(DIMENSION)])\n",
    "df_entropy.to_csv(os.path.join(OUTPUT_DIR_ENTROPY, f'entropy_analysis{file_suffix}.csv'), index=False)\n",
    "\n",
    "# 保存贡献度分析\n",
    "df_contribution = pd.DataFrame(results_contribution, columns=[f'contribution_{j}' for j in range(DIMENSION)])\n",
    "df_contribution.to_csv(os.path.join(OUTPUT_DIR_CONTRIBUTION, f'contribution_analysis{file_suffix}.csv'), index=False)\n",
    "\n",
    "# print(f\"所有结果已成功保存到目录: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
